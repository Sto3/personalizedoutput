/**
 * Redi V7 Server - PRODUCTION-GRADE RELIABILITY
 * =============================================
 * 
 * Complete rewrite for military-grade reliability.
 * Every edge case handled. Every state tracked.
 * 
 * KEY IMPROVEMENTS OVER V6:
 * 1. Response state machine (pending â†’ active â†’ complete)
 * 2. Barge-in handling with response.cancel
 * 3. Fresh frame requests at optimal moment
 * 4. Response queuing to prevent collisions
 * 5. Audio stop signal on interruption
 * 6. Higher image quality (768px, 85% JPEG)
 * 7. Stricter frame freshness (2 sec max)
 * 8. Enhanced system prompt for accuracy
 * 9. Graceful error recovery
 * 10. gpt-realtime GA model with VISION SUPPORT
 * 11. GA API nested audio format (fixed Jan 20)
 * 
 * Endpoint: /ws/redi?v=7
 */

import { WebSocketServer, WebSocket } from 'ws';
import { Server as HTTPServer, IncomingMessage } from 'http';
import { randomUUID } from 'crypto';

// =============================================================================
// CONFIGURATION
// =============================================================================

const OPENAI_API_KEY = process.env.OPENAI_API_KEY;

// CRITICAL: Use gpt-realtime (GA) instead of gpt-4o-realtime-preview
// The preview model does NOT support vision! Only gpt-realtime does.
const OPENAI_REALTIME_URL = 'wss://api.openai.com/v1/realtime?model=gpt-realtime';

// Frame quality settings - HIGHER for better vision accuracy
const MAX_FRAME_AGE_MS = 2000;  // Max 2 seconds old (was 5)
const REQUEST_FRESH_FRAME_DELAY_MS = 100;  // Wait 100ms for fresh frame

// =============================================================================
// TYPES
// =============================================================================

type ResponseState = 'idle' | 'pending' | 'active' | 'cancelling';

interface PendingResponse {
  transcript: string;
  timestamp: number;
}

interface Session {
  id: string;
  clientWs: WebSocket;
  openaiWs: WebSocket | null;
  
  // Frame management
  currentFrame: string | null;
  frameTimestamp: number;
  pendingFrameRequest: boolean;
  
  // Response state machine
  responseState: ResponseState;
  currentResponseId: string | null;
  pendingResponses: PendingResponse[];
  
  // Speaking states
  isUserSpeaking: boolean;
  lastTranscript: string;
  
  // Stats
  connectionTime: number;
  responsesCompleted: number;
  responsesCancelled: number;
  imagesInjected: number;
  errors: number;
}

// =============================================================================
// STATE
// =============================================================================

const sessions = new Map<string, Session>();
let wss: WebSocketServer | null = null;

// =============================================================================
// SYSTEM PROMPT - ACCURACY FOCUSED
// =============================================================================

const SYSTEM_PROMPT = `You are Redi, an AI assistant with real-time voice and vision.

CRITICAL RULES FOR VISION:
1. ONLY describe what you can ACTUALLY SEE in the image. Never guess or assume.
2. If you cannot clearly read text, say "I can see text but can't read it clearly"
3. If asked about something not visible, say "I don't see that in the current view"
4. Be SPECIFIC: "I see 5 items" not "I see several items"
5. For screens/UIs: describe layout, colors, icons, and any readable text
6. Express uncertainty: "I think that's..." or "It looks like..." when not 100% sure
7. NEVER make up details. If the image is blurry or unclear, say so.

WHEN NO IMAGE IS PROVIDED:
- Say "I don't have a camera view right now" 
- Never pretend to see something you can't

RESPONSE STYLE:
- Be concise (under 40 words unless detail is needed)
- Natural, conversational tone
- No filler phrases like "Great question!"
- Speak like a helpful friend

LANGUAGE: Always respond in English.

ACCURACY IS MORE IMPORTANT THAN SPEED. Take your time to describe correctly.`;

// =============================================================================
// INITIALIZATION
// =============================================================================

export async function initRediV7(server: HTTPServer): Promise<void> {
  console.log('[Redi V7] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('[Redi V7] ðŸš€ Starting V7 Server - PRODUCTION GRADE');
  console.log('[Redi V7] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');
  console.log('[Redi V7] Model: gpt-realtime (GA with VISION)');
  console.log('[Redi V7] Version: Jan 20 2026 - GA Audio Format Fix');
  console.log('[Redi V7] Features:');
  console.log('[Redi V7]   âœ“ Response state machine');
  console.log('[Redi V7]   âœ“ Barge-in with response.cancel');
  console.log('[Redi V7]   âœ“ Fresh frame requests');
  console.log('[Redi V7]   âœ“ 2-second max frame age');
  console.log('[Redi V7]   âœ“ Higher image quality');
  console.log('[Redi V7]   âœ“ IMAGE INPUT SUPPORTED (gpt-realtime GA)');
  console.log('[Redi V7]   âœ“ GA NESTED AUDIO FORMAT');
  console.log('[Redi V7] â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•');

  if (!OPENAI_API_KEY) {
    console.error('[Redi V7] âŒ OPENAI_API_KEY not set!');
    return;
  }

  wss = new WebSocketServer({ noServer: true });

  wss.on('connection', async (ws: WebSocket, req: IncomingMessage) => {
    const sessionId = randomUUID();
    console.log(`[Redi V7] ðŸ”Œ New connection: ${sessionId}`);

    const session: Session = {
      id: sessionId,
      clientWs: ws,
      openaiWs: null,
      currentFrame: null,
      frameTimestamp: 0,
      pendingFrameRequest: false,
      responseState: 'idle',
      currentResponseId: null,
      pendingResponses: [],
      isUserSpeaking: false,
      lastTranscript: '',
      connectionTime: Date.now(),
      responsesCompleted: 0,
      responsesCancelled: 0,
      imagesInjected: 0,
      errors: 0,
    };

    sessions.set(sessionId, session);

    try {
      await connectToOpenAI(session);
      sendToClient(session, { type: 'session_ready', sessionId });
      console.log(`[Redi V7] âœ… Session ${sessionId} ready`);
    } catch (error) {
      console.error(`[Redi V7] âŒ Failed to connect to OpenAI:`, error);
      ws.close(1011, 'OpenAI connection failed');
      return;
    }

    ws.on('message', (data: Buffer) => {
      try {
        const message = JSON.parse(data.toString());
        handleClientMessage(session, message);
      } catch (error) {
        console.error(`[Redi V7] Parse error:`, error);
      }
    });

    ws.on('close', (code) => {
      const duration = Math.round((Date.now() - session.connectionTime) / 1000);
      console.log(`[Redi V7] ðŸ”Œ Disconnected: ${sessionId} (${duration}s, ${session.responsesCompleted} responses, ${session.errors} errors)`);
      cleanup(sessionId);
    });

    ws.on('error', (error) => {
      console.error(`[Redi V7] Client error:`, error);
      cleanup(sessionId);
    });
  });

  console.log('[Redi V7] WebSocket server initialized on /ws/redi?v=7');
}

export function handleV7Upgrade(request: IncomingMessage, socket: any, head: Buffer): boolean {
  if (!wss) {
    console.error('[Redi V7] âŒ WSS not initialized');
    return false;
  }

  console.log(`[Redi V7] ðŸ”„ Handling upgrade for V7 connection`);
  wss.handleUpgrade(request, socket, head, (ws) => {
    wss!.emit('connection', ws, request);
  });
  return true;
}

// =============================================================================
// OPENAI CONNECTION
// =============================================================================

async function connectToOpenAI(session: Session): Promise<void> {
  return new Promise((resolve, reject) => {
    console.log(`[Redi V7] ðŸ”— Connecting to OpenAI (gpt-realtime GA)...`);

    // NOTE: For GA API, we do NOT include OpenAI-Beta header
    const ws = new WebSocket(OPENAI_REALTIME_URL, {
      headers: {
        'Authorization': `Bearer ${OPENAI_API_KEY}`
      }
    });

    ws.on('open', () => {
      console.log(`[Redi V7] âœ… Connected to OpenAI (gpt-realtime)`);
      session.openaiWs = ws;
      configureSession(session);
      resolve();
    });

    ws.on('message', (data: Buffer) => {
      handleOpenAIMessage(session, data);
    });

    ws.on('error', (error) => {
      console.error(`[Redi V7] âŒ OpenAI error:`, error);
      session.errors++;
      reject(error);
    });

    ws.on('close', (code) => {
      console.log(`[Redi V7] OpenAI closed: code=${code}`);
    });
  });
}

// =============================================================================
// SESSION CONFIGURATION - GA API NESTED AUDIO FORMAT
// =============================================================================

function configureSession(session: Session): void {
  // CRITICAL: GA API uses NESTED audio object, not flat properties
  // Beta format: input_audio_format: 'pcm16', voice: 'alloy'
  // GA format: audio.input.format.type, audio.output.voice
  const config = {
    type: 'session.update',
    session: {
      type: 'realtime',
      model: 'gpt-realtime',
      instructions: SYSTEM_PROMPT,
      output_modalities: ['audio'],
      audio: {
        input: {
          format: {
            type: 'audio/pcm',
            rate: 24000
          },
          transcription: {
            model: 'whisper-1'
          },
          turn_detection: {
            type: 'server_vad',
            threshold: 0.5,
            prefix_padding_ms: 300,
            silence_duration_ms: 500
          }
        },
        output: {
          format: {
            type: 'audio/pcm',
            rate: 24000
          },
          voice: 'alloy'
        }
      }
    }
  };

  console.log('[Redi V7] ðŸ”§ Configuring session (GA nested audio format)...');
  console.log('[Redi V7] ðŸ“‹ Config: type=realtime, model=gpt-realtime, voice=alloy');
  sendToOpenAI(session, config);
}

// =============================================================================
// CLIENT MESSAGE HANDLING
// =============================================================================

function handleClientMessage(session: Session, message: any): void {
  switch (message.type) {
    case 'audio':
      if (message.data) {
        sendToOpenAI(session, {
          type: 'input_audio_buffer.append',
          audio: message.data
        });
      }
      break;

    case 'frame':
      const frameSize = message.data?.length || 0;
      const frameSizeKB = Math.round(frameSize * 0.75 / 1024);
      session.currentFrame = message.data;
      session.frameTimestamp = Date.now();
      session.pendingFrameRequest = false;
      
      // Only log occasionally to reduce noise
      if (Math.random() < 0.1) {
        console.log(`[Redi V7] ðŸ“· Frame: ${frameSizeKB}KB`);
      }
      break;

    case 'mode':
    case 'sensitivity':
      // Acknowledged
      break;

    default:
      console.log(`[Redi V7] Unknown message: ${message.type}`);
  }
}

// =============================================================================
// OPENAI MESSAGE HANDLING - STATE MACHINE
// =============================================================================

function handleOpenAIMessage(session: Session, data: Buffer): void {
  try {
    const event = JSON.parse(data.toString());
    
    switch (event.type) {
      case 'session.created':
        console.log('[Redi V7] Session created');
        break;

      case 'session.updated':
        console.log('[Redi V7] âœ… Session configured successfully');
        // Log the effective config for debugging
        if (event.session) {
          const voice = event.session.audio?.output?.voice || event.session.voice || 'unknown';
          const model = event.session.model || 'unknown';
          console.log(`[Redi V7] ðŸ“‹ Effective config: model=${model}, voice=${voice}`);
        }
        break;

      case 'error':
        handleOpenAIError(session, event.error);
        break;

      case 'input_audio_buffer.speech_started':
        handleUserStartedSpeaking(session);
        break;

      case 'input_audio_buffer.speech_stopped':
        session.isUserSpeaking = false;
        console.log('[Redi V7] ðŸŽ¤ User stopped');
        break;

      case 'conversation.item.input_audio_transcription.completed':
        if (event.transcript) {
          handleTranscriptCompleted(session, event.transcript);
        }
        break;

      case 'response.created':
        session.responseState = 'active';
        session.currentResponseId = event.response?.id || null;
        sendToClient(session, { type: 'mute_mic', muted: true });
        break;

      // GA API uses response.output_audio.delta instead of response.audio.delta
      case 'response.audio.delta':
      case 'response.output_audio.delta':
        const audioData = event.delta || event.data;
        if (audioData) {
          sendToClient(session, { type: 'audio', data: audioData });
        }
        break;

      // GA API uses response.output_audio_transcript.done
      case 'response.audio_transcript.done':
      case 'response.output_audio_transcript.done':
        if (event.transcript) {
          console.log(`[Redi V7] ðŸ¤– Redi: "${event.transcript}"`);
          sendToClient(session, { type: 'transcript', text: event.transcript, role: 'assistant' });
        }
        break;

      case 'response.done':
        handleResponseDone(session, event);
        break;

      case 'response.cancelled':
        console.log('[Redi V7] ðŸ›‘ Response cancelled');
        session.responseState = 'idle';
        session.currentResponseId = null;
        session.responsesCancelled++;
        processNextPendingResponse(session);
        break;

      case 'conversation.item.created':
      case 'conversation.item.added':
        logConversationItem(event);
        break;

      case 'rate_limits.updated':
      case 'input_audio_buffer.committed':
      case 'input_audio_buffer.cleared':
        break;

      default:
        if (!event.type.includes('delta')) {
          // Log other events for debugging
        }
    }
  } catch (error) {
    console.error(`[Redi V7] Parse error:`, error);
    session.errors++;
  }
}

// =============================================================================
// STATE MACHINE HANDLERS
// =============================================================================

function handleUserStartedSpeaking(session: Session): void {
  session.isUserSpeaking = true;
  console.log('[Redi V7] ðŸŽ¤ User speaking...');
  
  // BARGE-IN: If Redi is currently responding, cancel it
  if (session.responseState === 'active' && session.currentResponseId) {
    console.log('[Redi V7] ðŸ›‘ BARGE-IN: Cancelling current response');
    session.responseState = 'cancelling';
    
    // Tell client to stop playing audio immediately
    sendToClient(session, { type: 'stop_audio' });
    
    // Cancel the response in OpenAI
    sendToOpenAI(session, {
      type: 'response.cancel'
    });
  }
  
  // Request a fresh frame NOW so it's ready when user stops speaking
  requestFreshFrame(session);
}

function handleTranscriptCompleted(session: Session, transcript: string): void {
  session.lastTranscript = transcript;
  console.log(`[Redi V7] ðŸ‘¤ User: "${transcript}"`);
  sendToClient(session, { type: 'transcript', text: transcript, role: 'user' });
  
  // Queue this response
  const pendingResponse: PendingResponse = {
    transcript,
    timestamp: Date.now()
  };
  
  // If we're idle, process immediately
  if (session.responseState === 'idle') {
    processResponse(session, pendingResponse);
  } else {
    // Queue it - will process when current response completes
    console.log(`[Redi V7] ðŸ“‹ Queued response (state: ${session.responseState})`);
    session.pendingResponses.push(pendingResponse);
  }
}

function handleResponseDone(session: Session, event: any): void {
  session.responseState = 'idle';
  session.currentResponseId = null;
  session.responsesCompleted++;
  
  sendToClient(session, { type: 'mute_mic', muted: false });
  console.log('[Redi V7] âœ… Response complete');
  
  // Process any queued responses
  processNextPendingResponse(session);
}

function handleOpenAIError(session: Session, error: any): void {
  const errorCode = error?.code || 'unknown';
  const errorMsg = error?.message || 'Unknown error';
  
  console.error(`[Redi V7] âŒ OpenAI Error [${errorCode}]: ${errorMsg}`);
  session.errors++;
  
  // Handle specific errors
  if (errorCode === 'conversation_already_has_active_response') {
    // This shouldn't happen with our state machine, but recover gracefully
    console.log('[Redi V7] âš ï¸ Response collision - waiting for current to complete');
    // Don't change state - let the current response complete
  } else if (errorCode === 'image_input_not_supported') {
    // This means we're using the wrong model - should not happen with gpt-realtime
    console.error('[Redi V7] âŒ CRITICAL: Model does not support images! Check OPENAI_REALTIME_URL');
    sendToClient(session, { type: 'error', message: 'Vision not available - wrong model' });
  } else if (errorCode === 'invalid_session_update') {
    // Session config format issue - log full error
    console.error('[Redi V7] âŒ Invalid session config:', JSON.stringify(error, null, 2));
  } else {
    sendToClient(session, { type: 'error', message: errorMsg });
  }
}

// =============================================================================
// RESPONSE PROCESSING
// =============================================================================

function processNextPendingResponse(session: Session): void {
  if (session.pendingResponses.length === 0) return;
  if (session.responseState !== 'idle') return;
  
  const next = session.pendingResponses.shift()!;
  const age = Date.now() - next.timestamp;
  
  // Skip if too old (user probably moved on)
  if (age > 10000) {
    console.log(`[Redi V7] â­ï¸ Skipping stale queued response (${age}ms old)`);
    processNextPendingResponse(session);
    return;
  }
  
  processResponse(session, next);
}

function processResponse(session: Session, pending: PendingResponse): void {
  session.responseState = 'pending';
  
  // Inject image if we have a fresh one
  const injected = maybeInjectImage(session, pending.transcript);
  
  // Trigger response
  console.log(`[Redi V7] ðŸ“¤ Triggering response (image: ${injected})`);
  sendToOpenAI(session, { type: 'response.create' });
}

// =============================================================================
// IMAGE INJECTION - FRESH FRAMES ONLY
// =============================================================================

function requestFreshFrame(session: Session): void {
  if (!session.pendingFrameRequest) {
    session.pendingFrameRequest = true;
    sendToClient(session, { type: 'request_frame' });
  }
}

function maybeInjectImage(session: Session, transcript: string): boolean {
  // Check if we have a frame
  if (!session.currentFrame) {
    console.log('[Redi V7] ðŸ“· No frame available');
    return false;
  }

  // Check frame age - STRICT 2 second limit
  const frameAge = Date.now() - session.frameTimestamp;
  if (frameAge > MAX_FRAME_AGE_MS) {
    console.log(`[Redi V7] ðŸ“· Frame too old (${frameAge}ms > ${MAX_FRAME_AGE_MS}ms)`);
    return false;
  }

  const cleanBase64 = session.currentFrame.replace(/[\r\n\s]/g, '');
  const sizeKB = Math.round(cleanBase64.length * 0.75 / 1024);

  console.log(`[Redi V7] ðŸ“· Injecting image: ${sizeKB}KB, age ${frameAge}ms`);

  // Create conversation item with image
  // Format matches GA API: type: 'input_image' with image_url containing data URI
  const imageItem = {
    type: 'conversation.item.create',
    item: {
      type: 'message',
      role: 'user',
      content: [
        {
          type: 'input_text',
          text: `[Camera view attached - user said: "${transcript}"]`
        },
        {
          type: 'input_image',
          image_url: `data:image/jpeg;base64,${cleanBase64}`
        }
      ]
    }
  };

  sendToOpenAI(session, imageItem);
  session.imagesInjected++;
  return true;
}

function logConversationItem(event: any): void {
  const contentTypes = event.item?.content?.map((c: any) => c.type) || [];
  if (contentTypes.length > 0) {
    console.log(`[Redi V7] ðŸ“¥ Item created: [${contentTypes.join(', ')}]`);
    if (contentTypes.includes('input_image')) {
      console.log(`[Redi V7] âœ… IMAGE ACCEPTED BY MODEL`);
    }
  }
}

// =============================================================================
// WEBSOCKET HELPERS
// =============================================================================

function sendToOpenAI(session: Session, message: any): void {
  if (session.openaiWs?.readyState === WebSocket.OPEN) {
    session.openaiWs.send(JSON.stringify(message));
  }
}

function sendToClient(session: Session, message: any): void {
  if (session.clientWs.readyState === WebSocket.OPEN) {
    session.clientWs.send(JSON.stringify(message));
  }
}

// =============================================================================
// CLEANUP
// =============================================================================

function cleanup(sessionId: string): void {
  const session = sessions.get(sessionId);
  if (session) {
    session.openaiWs?.close();
    sessions.delete(sessionId);
    console.log(`[Redi V7] Cleaned up: ${sessionId}`);
  }
}

export function closeRediV7(): void {
  sessions.forEach((_, id) => cleanup(id));
  if (wss) {
    wss.close();
    wss = null;
    console.log('[Redi V7] Server closed');
  }
}

export function getV7Stats(): object {
  const sessionStats = Array.from(sessions.values()).map(s => ({
    id: s.id.slice(0, 8),
    uptime: Math.round((Date.now() - s.connectionTime) / 1000),
    responses: s.responsesCompleted,
    cancelled: s.responsesCancelled,
    images: s.imagesInjected,
    errors: s.errors,
    state: s.responseState
  }));
  
  return {
    activeSessions: sessions.size,
    sessions: sessionStats
  };
}
